# Import modules
from ..logger import log_message
from ..db_write import update_session_status, save_to_db, cleanup_incomplete
from .scraper import scrape_categories, scrape_catalog_page, scrape_product_page


def parse_catalog(
    catalog_url=None,
    category=None,
    max_pages=None,
    max_products=None,
    session_id=None,
    url=None,
    cancel_flags=None,
    static_folder=None,
):
    log_message(
        session_id,
        f"–ù–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞: URL={url}, –ö–∞—Ç–µ–≥–æ—Ä–∏—è={category}, –ö–∞—Ç–∞–ª–æ–≥={catalog_url}",
        level="debug",
    )
    update_session_status(session_id, "in_progress", progress="collecting_urls")

    try:
        if url and "/products/" in url:
            # Parse single product
            product = scrape_product_page(
                url, category or "Unknown", session_id, cancel_flags, static_folder
            )
            if product and save_to_db(
                product, product.variants, session_id, cancel_flags
            ):
                log_message(
                    session_id,
                    f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –ø—Ä–æ–¥—É–∫—Ç: {product.title}",
                    level="info",
                )
            else:
                log_message(
                    session_id,
                    f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–æ–¥—É–∫—Ç–∞: {url}",
                    level="error",
                )
        elif url and "/collection/" in url:
            # Parse specific category via URL
            product_urls = scrape_catalog_page(
                url,
                category or "Unknown",
                max_pages,
                max_products,
                session_id,
                cancel_flags,
            )
            log_message(
                session_id,
                f"Found {len(product_urls)} product URLs in category",
                level="debug",
            )
            if len(product_urls) > 5:
                update_session_status(
                    session_id,
                    "awaiting_confirmation",
                    product_urls,
                    "awaiting_confirmation",
                )
                log_message(
                    session_id,
                    f"üõë –ù–∞–π–¥–µ–Ω–æ {len(product_urls)} –ø—Ä–æ–¥—É–∫—Ç–æ–≤, —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ",
                    level="warning",
                )
                return
            parse_product_urls(
                product_urls, category or "Unknown", session_id, cancel_flags
            )
        elif catalog_url:
            # Parse selected category
            product_urls = scrape_catalog_page(
                catalog_url, category, max_pages, max_products, session_id, cancel_flags
            )
            log_message(
                session_id,
                f"Found {len(product_urls, level="debug")} product URLs in catalog",
                level="info",
            )
            if len(product_urls) > 5:
                update_session_status(
                    session_id,
                    "awaiting_confirmation",
                    product_urls,
                    "awaiting_confirmation",
                )
                log_message(
                    session_id,
                    f"üõë –ù–∞–π–¥–µ–Ω–æ {len(product_urls)} –ø—Ä–æ–¥—É–∫—Ç–æ–≤, —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ",
                    level="warning",
                )
                return
            parse_product_urls(product_urls, category, session_id, cancel_flags)
        else:
            # Parse all categories
            categories = scrape_categories()
            log_message(
                session_id,
                f"Found {len(categories, level="debug")} categories to parse",
                level="info",
            )
            total_products = 0
            product_urls = []
            for cat in categories:
                if max_products and total_products >= max_products:
                    log_message(
                        session_id,
                        f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –ø—Ä–æ–¥—É–∫—Ç–æ–≤ ({max_products})",
                        level="debug",
                    )
                    break
                if cancel_flags.get(session_id, False):
                    log_message(
                        session_id,
                        "‚ö†Ô∏è –ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–º–µ–Ω–µ–Ω, –ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∏–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π",
                        level="warning",
                    )
                    break
                cat_urls = scrape_catalog_page(
                    cat["url"],
                    cat["name"],
                    max_pages,
                    max_products - total_products,
                    session_id,
                    cancel_flags,
                )
                product_urls.extend([(url, cat["name"]) for url in cat_urls])
                total_products += len(cat_urls)
            if total_products > 5:
                update_session_status(
                    session_id,
                    "awaiting_confirmation",
                    product_urls,
                    "awaiting_confirmation",
                )
                log_message(
                    session_id,
                    f"üõë –ù–∞–π–¥–µ–Ω–æ {total_products} –ø—Ä–æ–¥—É–∫—Ç–æ–≤, —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ",
                    level="warning",
                )
                return
            parse_product_urls(product_urls, None, session_id, cancel_flags)
    except Exception as e:
        log_message(session_id, f"‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}", level="error")
        update_session_status(session_id, "error")
        cleanup_incomplete(session_id)


def parse_product_urls(product_urls, category, session_id, cancel_flags, static_folder):
    log_message(
        session_id, f"Processing {len(product_urls)} product URLs", level="debug"
    )
    for item in product_urls:
        if cancel_flags.get(session_id, False):
            log_message(
                session_id,
                "‚ö†Ô∏è –ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–º–µ–Ω–µ–Ω, –ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤",
                level="warning",
            )
            break
        url = item[0] if isinstance(item, tuple) else item
        cat = item[1] if isinstance(item, tuple) else category
        try:
            product = scrape_product_page(
                url, cat, session_id, cancel_flags, static_folder
            )
            if product and save_to_db(
                product, product.variants, session_id, cancel_flags
            ):
                log_message(
                    session_id,
                    f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {product.to_dict()}",
                    level="info",
                )
            else:
                log_message(
                    session_id,
                    f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–æ–¥—É–∫—Ç–∞: {url}",
                    level="error",
                )
        except Exception as e:
            log_message(
                session_id,
                f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–æ–¥—É–∫—Ç–∞ {url}: {e}",
                level="error",
            )
